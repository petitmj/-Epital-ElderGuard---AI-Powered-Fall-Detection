{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fall Detection Inference Pipeline - Model Deployment\n",
    "\n",
    "## Overview\n",
    "This notebook converts trained ML models into formats optimized for mobile and edge devices. It prepares models for real-world deployment where computational resources are limited.\n",
    "\n",
    "## Deployment Targets\n",
    "1. **Mobile Devices** (Android/iOS) - TFLite format\n",
    "2. **Cross-Platform** - ONNX format\n",
    "3. **Qualcomm Devices** - Optimized for Snapdragon processors\n",
    "\n",
    "## Optimization Techniques\n",
    "- **Quantization**: Reduce model size by 75% (32-bit ‚Üí 8-bit)\n",
    "- **Hardware Acceleration**: Leverage NPU/GPU on mobile devices\n",
    "- **Model Compression**: Remove unnecessary operations\n",
    "\n",
    "## Why This Matters\n",
    "- **Latency**: Falls need detection within milliseconds\n",
    "- **Battery**: Models run continuously on wearables\n",
    "- **Privacy**: On-device processing, no cloud dependency\n",
    "- **Reliability**: Works offline without network connectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Import Required Libraries\n",
    "\n",
    "### Purpose\n",
    "Import specialized libraries for model conversion and deployment.\n",
    "\n",
    "### Key Libraries\n",
    "- **tf.lite**: Convert TensorFlow models to mobile-friendly format\n",
    "- **skl2onnx**: Convert scikit-learn models to ONNX\n",
    "- **onnxruntime**: Run ONNX models\n",
    "- **qai_hub**: Deploy to Qualcomm Snapdragon devices\n",
    "- **tf2onnx**: Convert TensorFlow/Keras to ONNX\n",
    "\n",
    "### Installation\n",
    "If missing, install with:\n",
    "```bash\n",
    "pip install tensorflow scikit-learn onnx onnxruntime skl2onnx tf2onnx qai-hub-models\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/attr_value.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/resource_handle.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_shape.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/types.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/full_type.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/function.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/node_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/op_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph_debug_info.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/versions.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at xla/tsl/protobuf/coordination_config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/cost_graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/step_stats.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/allocation_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/cluster.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/debug.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tf2onnx'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msaving\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m register_keras_serializable\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlosses\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MeanSquaredError\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf2onnx\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tf2onnx'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import onnxruntime as ort\n",
    "import onnx\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "import qai_hub as hub\n",
    "from keras.saving import register_keras_serializable\n",
    "from keras.losses import MeanSquaredError\n",
    "import tf2onnx\n",
    "import os\n",
    "import time\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"ONNX Runtime Version: {ort.__version__}\")\n",
    "print(f\"ONNX Version: {onnx.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Load Pre-trained Autoencoder\n",
    "\n",
    "### Purpose\n",
    "Load the autoencoder model trained in the previous pipeline.\n",
    "\n",
    "### Expected File\n",
    "`fall_detection_model.h5` or `autoencoder.h5` from training pipeline\n",
    "\n",
    "### Fallback Behavior\n",
    "If no pre-trained model exists, creates a simple autoencoder for demonstration.\n",
    "\n",
    "### Architecture Reminder\n",
    "```\n",
    "Input (N features) \n",
    "  ‚Üì\n",
    "Dense(16) + ReLU   ‚Üê Encoder\n",
    "  ‚Üì\n",
    "Dense(8) + ReLU    ‚Üê Bottleneck (compressed)\n",
    "  ‚Üì\n",
    "Dense(16) + ReLU   ‚Üê Decoder\n",
    "  ‚Üì\n",
    "Dense(N) + Sigmoid ‚Üê Reconstruction\n",
    "```\n",
    "\n",
    "### Production Note\n",
    "Always use the actual trained model, not the fallback!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"LOADING AUTOENCODER MODEL\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Try to load pre-trained model\n",
    "model_paths = [\"autoencoder.h5\", \"fall_detection_model.h5\"]\n",
    "autoencoder = None\n",
    "\n",
    "for path in model_paths:\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            autoencoder = keras.models.load_model(path)\n",
    "            print(f\"‚úÖ Loaded existing autoencoder from: {path}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Failed to load {path}: {e}\")\n",
    "\n",
    "# Fallback: Create simple autoencoder for demonstration\n",
    "if autoencoder is None:\n",
    "    print(\"\\nüö® No pre-trained model found. Creating demonstration autoencoder...\")\n",
    "    print(\"‚ö†Ô∏è WARNING: Use the actual trained model in production!\\n\")\n",
    "    \n",
    "    input_dim = 10  # Adjust based on your feature size\n",
    "    input_layer = keras.layers.Input(shape=(input_dim,))\n",
    "    encoded = keras.layers.Dense(8, activation=\"relu\")(input_layer)\n",
    "    decoded = keras.layers.Dense(input_dim, activation=\"sigmoid\")(encoded)\n",
    "    \n",
    "    autoencoder = keras.models.Model(input_layer, decoded)\n",
    "    autoencoder.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "    \n",
    "    # Save for consistency\n",
    "    autoencoder.save(\"fall_detection_model.h5\")\n",
    "    print(\"‚úÖ Demonstration autoencoder created and saved.\")\n",
    "\n",
    "# Display model information\n",
    "print(f\"\\nModel Architecture:\")\n",
    "print(f\"  Input shape: {autoencoder.input_shape}\")\n",
    "print(f\"  Output shape: {autoencoder.output_shape}\")\n",
    "print(f\"  Total parameters: {autoencoder.count_params():,}\")\n",
    "\n",
    "print(f\"\\nDetailed Architecture:\")\n",
    "autoencoder.summary()\n",
    "\n",
    "# Calculate model size\n",
    "if os.path.exists(\"fall_detection_model.h5\"):\n",
    "    size_mb = os.path.getsize(\"fall_detection_model.h5\") / (1024 * 1024)\n",
    "    print(f\"\\nModel file size: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Convert to TensorFlow Lite (TFLite)\n",
    "\n",
    "### What is TFLite?\n",
    "TensorFlow Lite is a lightweight ML framework designed for mobile and embedded devices.\n",
    "\n",
    "### Conversion Process\n",
    "1. Takes Keras model\n",
    "2. Optimizes graph (removes training-only ops)\n",
    "3. Converts to FlatBuffer format\n",
    "4. Produces `.tflite` file\n",
    "\n",
    "### Benefits\n",
    "- **Smaller size**: Typically 50-75% reduction\n",
    "- **Faster inference**: Optimized for mobile CPUs\n",
    "- **Hardware acceleration**: Can use GPU, DSP, or NPU\n",
    "- **Cross-platform**: Works on Android, iOS, Raspberry Pi, etc.\n",
    "\n",
    "### Trade-offs\n",
    "- Limited op support (some layers unsupported)\n",
    "- Slightly lower accuracy (minor)\n",
    "- No training capability (inference only)\n",
    "\n",
    "### Use Cases\n",
    "- Real-time fall detection on smartphones\n",
    "- Wearable devices (smartwatches)\n",
    "- IoT sensors with edge computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"CONVERTING TO TENSORFLOW LITE\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Initialize TFLite converter\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(autoencoder)\n",
    "\n",
    "# Optional: Enable optimizations (uncomment for further size reduction)\n",
    "# converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# Convert model\n",
    "print(\"Converting model...\")\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save TFLite model\n",
    "tflite_path = \"fall_detection_model.tflite\"\n",
    "with open(tflite_path, \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(f\"‚úÖ Autoencoder successfully converted to TFLite\")\n",
    "\n",
    "# Compare file sizes\n",
    "h5_size = os.path.getsize(\"fall_detection_model.h5\") / 1024\n",
    "tflite_size = len(tflite_model) / 1024\n",
    "reduction = (1 - tflite_size / h5_size) * 100\n",
    "\n",
    "print(f\"\\nFile Size Comparison:\")\n",
    "print(f\"  Original (.h5): {h5_size:.2f} KB\")\n",
    "print(f\"  TFLite (.tflite): {tflite_size:.2f} KB\")\n",
    "print(f\"  Size reduction: {reduction:.1f}%\")\n",
    "\n",
    "print(f\"\\nSaved as: {tflite_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Train and Prepare Isolation Forest\n",
    "\n",
    "### Purpose\n",
    "Create an Isolation Forest model for ONNX conversion demonstration.\n",
    "\n",
    "### Production Note\n",
    "**Replace this synthetic training** with loading your actual pre-trained Isolation Forest:\n",
    "```python\n",
    "import joblib\n",
    "model = joblib.load('isolation_forest.pkl')\n",
    "```\n",
    "\n",
    "### Isolation Forest Overview\n",
    "- **Type**: Unsupervised anomaly detection\n",
    "- **Method**: Builds random decision trees\n",
    "- **Logic**: Anomalies are easier to isolate (fewer splits)\n",
    "- **Speed**: Very fast training and inference\n",
    "\n",
    "### Parameters\n",
    "- `contamination=0.05`: Expects 5% of data to be anomalies\n",
    "- `max_features=10`: Uses 10 features for each tree\n",
    "- `random_state=42`: Reproducibility\n",
    "\n",
    "### ONNX Compatibility Fix\n",
    "The `_max_features` attribute is required for ONNX conversion but sometimes missing. We add it manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"PREPARING ISOLATION FOREST FOR ONNX CONVERSION\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Check if pre-trained model exists\n",
    "if os.path.exists('isolation_forest.pkl'):\n",
    "    print(\"‚úÖ Loading pre-trained Isolation Forest...\")\n",
    "    import joblib\n",
    "    model = joblib.load('isolation_forest.pkl')\n",
    "    print(\"‚úÖ Pre-trained model loaded successfully\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No pre-trained model found. Training demonstration model...\")\n",
    "    print(\"‚ö†Ô∏è WARNING: Use actual trained model in production!\\n\")\n",
    "    \n",
    "    # Generate synthetic training data\n",
    "    # In production: use actual sensor data\n",
    "    train_data = np.random.rand(100, 10).astype(np.float32)\n",
    "    \n",
    "    # Train Isolation Forest\n",
    "    model = IsolationForest(\n",
    "        contamination=0.05,    # 5% anomalies expected\n",
    "        random_state=42,\n",
    "        max_features=10,       # Feature count\n",
    "        n_estimators=100,      # Number of trees\n",
    "        n_jobs=-1              # Use all cores\n",
    "    )\n",
    "    \n",
    "    print(f\"Training on {train_data.shape[0]} samples...\")\n",
    "    model.fit(train_data)\n",
    "    print(\"‚úÖ Model trained\")\n",
    "\n",
    "# Fix for ONNX conversion compatibility\n",
    "if not hasattr(model, \"_max_features\"):\n",
    "    model._max_features = model.max_features_\n",
    "    print(\"‚úÖ Added _max_features attribute for ONNX compatibility\")\n",
    "\n",
    "# Display model information\n",
    "print(f\"\\nModel Configuration:\")\n",
    "print(f\"  Number of estimators: {model.n_estimators}\")\n",
    "print(f\"  Contamination: {model.contamination}\")\n",
    "print(f\"  Max features: {model.max_features}\")\n",
    "\n",
    "# Get feature count for later use\n",
    "num_features = model.max_features if isinstance(model.max_features, int) else 10\n",
    "print(f\"  Feature dimensions: {num_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Convert Isolation Forest to ONNX\n",
    "\n",
    "### What is ONNX?\n",
    "Open Neural Network Exchange - an open format for representing ML models.\n",
    "\n",
    "### Why ONNX?\n",
    "- **Interoperability**: Train in scikit-learn, deploy anywhere\n",
    "- **Performance**: Optimized runtime for inference\n",
    "- **Flexibility**: Works across frameworks (PyTorch, TensorFlow, scikit-learn)\n",
    "- **Hardware support**: CPU, GPU, NPU, TPU\n",
    "\n",
    "### Conversion Steps\n",
    "1. Define input schema (shape and type)\n",
    "2. Convert scikit-learn model to ONNX graph\n",
    "3. Set target opset version (controls available operations)\n",
    "4. Serialize to `.onnx` file\n",
    "\n",
    "### Opset Versions\n",
    "- **Standard opset (15)**: Core operations\n",
    "- **ML opset (3)**: ML-specific operations (trees, SVMs, etc.)\n",
    "\n",
    "### File Format\n",
    "ONNX uses Protocol Buffers for efficient serialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"CONVERTING ISOLATION FOREST TO ONNX\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Define input type for ONNX conversion\n",
    "# [None, num_features] means variable batch size, fixed feature count\n",
    "initial_type = [(\"input\", FloatTensorType([None, num_features]))]\n",
    "\n",
    "print(f\"Input specification: FloatTensor[None, {num_features}]\")\n",
    "print(\"  - None: Variable batch size (can process 1 or more samples)\")\n",
    "print(f\"  - {num_features}: Fixed number of features\\n\")\n",
    "\n",
    "# Convert model to ONNX format\n",
    "print(\"Converting scikit-learn model to ONNX...\")\n",
    "onnx_model = convert_sklearn(\n",
    "    model, \n",
    "    initial_types=initial_type,\n",
    "    target_opset={\n",
    "        \"\": 15,        # Standard ONNX opset version 15\n",
    "        \"ai.onnx.ml\": 3  # ML-specific opset version 3\n",
    "    }\n",
    ")\n",
    "\n",
    "# Save ONNX model\n",
    "onnx_path = \"fall_detection_model.onnx\"\n",
    "with open(onnx_path, \"wb\") as f:\n",
    "    f.write(onnx_model.SerializeToString())\n",
    "\n",
    "print(f\"‚úÖ IsolationForest successfully converted to ONNX\")\n",
    "\n",
    "# Display ONNX model information\n",
    "onnx_size = os.path.getsize(onnx_path) / 1024\n",
    "print(f\"\\nONNX Model Information:\")\n",
    "print(f\"  File size: {onnx_size:.2f} KB\")\n",
    "print(f\"  Saved as: {onnx_path}\")\n",
    "print(f\"  IR version: {onnx_model.ir_version}\")\n",
    "print(f\"  Producer: {onnx_model.producer_name}\")\n",
    "\n",
    "# Verify model\n",
    "try:\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "    print(\"  ‚úÖ Model validation passed\")\n",
    "except Exception as e:\n",
    "    print(f\"  ‚ö†Ô∏è Model validation warning: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Test TFLite Model Inference\n",
    "\n",
    "### Purpose\n",
    "Validate that the converted TFLite model works correctly.\n",
    "\n",
    "### TFLite Inference Steps\n",
    "1. **Load interpreter**: Reads the `.tflite` file\n",
    "2. **Allocate tensors**: Reserves memory for inputs/outputs\n",
    "3. **Set input**: Provide sample data\n",
    "4. **Invoke**: Run inference\n",
    "5. **Get output**: Retrieve predictions\n",
    "\n",
    "### Tensor Details\n",
    "- **Input tensor**: Shape, dtype, index\n",
    "- **Output tensor**: Shape, dtype, index\n",
    "\n",
    "### Why Test?\n",
    "- Verify conversion didn't break the model\n",
    "- Check output shapes match expectations\n",
    "- Ensure dtype compatibility\n",
    "- Measure inference latency\n",
    "\n",
    "### Mobile Integration\n",
    "On Android/iOS, you'll use similar APIs:\n",
    "- **Android**: TensorFlow Lite Java/Kotlin API\n",
    "- **iOS**: TensorFlow Lite Swift API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"TESTING TFLITE MODEL INFERENCE\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Load TFLite model\n",
    "interpreter = tf.lite.Interpreter(model_path=\"fall_detection_model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "print(\"‚úÖ TFLite interpreter loaded and tensors allocated\")\n",
    "\n",
    "# Get input and output tensor details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print(f\"\\nInput Tensor Details:\")\n",
    "print(f\"  Name: {input_details[0]['name']}\")\n",
    "print(f\"  Shape: {input_details[0]['shape']}\")\n",
    "print(f\"  Data type: {input_details[0]['dtype']}\")\n",
    "print(f\"  Index: {input_details[0]['index']}\")\n",
    "\n",
    "print(f\"\\nOutput Tensor Details:\")\n",
    "print(f\"  Name: {output_details[0]['name']}\")\n",
    "print(f\"  Shape: {output_details[0]['shape']}\")\n",
    "print(f\"  Data type: {output_details[0]['dtype']}\")\n",
    "print(f\"  Index: {output_details[0]['index']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample input matching the model's expected shape\n",
    "input_shape = input_details[0]['shape']\n",
    "sample_data = np.random.rand(*input_shape).astype(np.float32)\n",
    "\n",
    "print(f\"\\nRunning inference...\")\n",
    "print(f\"  Input shape: {sample_data.shape}\")\n",
    "print(f\"  Input sample (first 5 values): {sample_data[0][:5]}\")\n",
    "\n",
    "# Measure inference time\n",
    "import time\n",
    "\n",
    "# Set input tensor\n",
    "interpreter.set_tensor(input_details[0]['index'], sample_data)\n",
    "\n",
    "# Run inference with timing\n",
    "start_time = time.time()\n",
    "interpreter.invoke()\n",
    "inference_time = (time.time() - start_time) * 1000  # Convert to ms\n",
    "\n",
    "# Get output tensor\n",
    "output = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "print(f\"\\n‚úÖ TFLite Inference Successful!\")\n",
    "print(f\"  Output shape: {output.shape}\")\n",
    "print(f\"  Output sample (first 5 values): {output[0][:5]}\")\n",
    "print(f\"  Inference time: {inference_time:.2f} ms\")\n",
    "\n",
    "# Performance analysis\n",
    "if inference_time < 10:\n",
    "    print(f\"  ‚úÖ Excellent latency for real-time fall detection\")\n",
    "elif inference_time < 50:\n",
    "    print(f\"  ‚úÖ Good latency for most applications\")\n",
    "else:\n",
    "    print(f\"  ‚ö†Ô∏è High latency - consider quantization or pruning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7: Test ONNX Model Inference\n",
    "\n",
    "### Purpose\n",
    "Validate the ONNX model works correctly with ONNX Runtime.\n",
    "\n",
    "### ONNX Runtime\n",
    "- High-performance inference engine\n",
    "- Cross-platform (Windows, Linux, macOS, Android, iOS)\n",
    "- Hardware accelerators (CPU, CUDA, TensorRT, DirectML)\n",
    "- Production-grade with Microsoft backing\n",
    "\n",
    "### Inference Process\n",
    "1. Create InferenceSession\n",
    "2. Get input/output specifications\n",
    "3. Prepare input data\n",
    "4. Run inference\n",
    "5. Process outputs\n",
    "\n",
    "### Isolation Forest Output\n",
    "- **Labels**: 1 (normal) or -1 (anomaly)\n",
    "- **Scores**: Anomaly scores (lower = more anomalous)\n",
    "\n",
    "### Use Cases\n",
    "- Web applications (via ONNX.js)\n",
    "- Server-side inference\n",
    "- Embedded Linux devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"TESTING ONNX MODEL INFERENCE\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Load ONNX model\n",
    "onnx_model_path = \"fall_detection_model.onnx\"\n",
    "session = ort.InferenceSession(onnx_model_path)\n",
    "print(\"‚úÖ ONNX Runtime session created\")\n",
    "\n",
    "# Get model metadata\n",
    "print(f\"\\nModel Metadata:\")\n",
    "print(f\"  Producer: {session.get_modelmeta().producer_name}\")\n",
    "print(f\"  Version: {session.get_modelmeta().version}\")\n",
    "\n",
    "# Get input specifications\n",
    "input_info = session.get_inputs()[0]\n",
    "input_name = input_info.name\n",
    "input_shape = input_info.shape\n",
    "input_type = input_info.type\n",
    "\n",
    "print(f\"\\nInput Specifications:\")\n",
    "print(f\"  Name: {input_name}\")\n",
    "print(f\"  Shape: {input_shape}\")\n",
    "print(f\"  Type: {input_type}\")\n",
    "\n",
    "# Get output specifications\n",
    "print(f\"\\nOutput Specifications:\")\n",
    "for i, output_info in enumerate(session.get_outputs()):\n",
    "    print(f\"  Output {i}:\")\n",
    "    print(f\"    Name: {output_info.name}\")\n",
    "    print(f\"    Shape: {output_info.shape}\")\n",
    "    print(f\"    Type: {output_info.type}\")\n",
    "\n",
    "# Extract feature count from shape\n",
    "num_features = input_shape[1] if len(input_shape) > 1 and input_shape[1] is not None else 10\n",
    "print(f\"\\nDetected feature count: {num_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample input data\n",
    "sample_input = np.random.rand(1, num_features).astype(np.float32)\n",
    "\n",
    "print(f\"\\nRunning inference...\")\n",
    "print(f\"  Input shape: {sample_input.shape}\")\n",
    "print(f\"  Input sample (first 5 values): {sample_input[0][:5]}\")\n",
    "\n",
    "# Run inference with timing\n",
    "start_time = time.time()\n",
    "output = session.run(None, {input_name: sample_input})\n",
    "inference_time = (time.time() - start_time) * 1000  # Convert to ms\n",
    "\n",
    "# Extract and print predictions\n",
    "# output[0] = labels, output[1] = scores\n",
    "pred_label = output[0][0] # Get the first label from the batch\n",
    "pred_score = output[1][0] # Get the first score from the batch\n",
    "\n",
    "print(f\"\\n‚úÖ ONNX Inference Successful!\")\n",
    "print(f\"  Inference time: {inference_time:.2f} ms\")\n",
    "print(f\"  Prediction Label: {pred_label} (1 = Normal, -1 = Anomaly)\")\n",
    "print(f\"  Anomaly Score: {pred_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 8: Optimize ONNX with Quantization\n",
    "\n",
    "### What is Quantization?\n",
    "Reduces numerical precision of weights and activations from 32-bit floats to 8-bit integers.\n",
    "\n",
    "### Dynamic Quantization\n",
    "- **Weights**: Converted to INT8 at conversion time\n",
    "- **Activations**: Quantized dynamically during inference\n",
    "- **Benefits**: No calibration data needed\n",
    "\n",
    "### Impact\n",
    "- **Model size**: ~75% reduction (4x smaller)\n",
    "- **Inference speed**: 2-4x faster on mobile CPUs\n",
    "- **Accuracy**: Typically <1% degradation\n",
    "- **Memory**: Lower RAM usage\n",
    "\n",
    "### When to Use\n",
    "- Deploying to mobile devices\n",
    "- Limited storage/bandwidth\n",
    "- Battery-powered devices\n",
    "- Real-time requirements\n",
    "\n",
    "### Trade-offs\n",
    "- Slight accuracy loss (usually negligible)\n",
    "- Not all operations supported\n",
    "- May not benefit on GPUs (optimized for FP32)\n",
    "\n",
    "### Quantization Types\n",
    "- **QInt8**: 8-bit signed integers (-128 to 127)\n",
    "- **QUInt8**: 8-bit unsigned integers (0 to 255)\n",
    "- We use QInt8 for better range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"OPTIMIZING ONNX MODEL WITH QUANTIZATION\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Define paths\n",
    "optimized_model_path = \"fall_detection_model_optimized.onnx\"\n",
    "\n",
    "print(\"Applying dynamic quantization...\")\n",
    "print(\"  Weight type: QInt8 (8-bit signed integers)\")\n",
    "print(\"  This will reduce model size by ~75%\\n\")\n",
    "\n",
    "# Apply quantization\n",
    "try:\n",
    "    quantize_dynamic(\n",
    "        model_input=onnx_model_path,\n",
    "        model_output=optimized_model_path,\n",
    "        weight_type=QuantType.QInt8,\n",
    "        op_types_to_quantize=['MatMul', 'Gemm']  # Common operations to quantize\n",
    "    )\n",
    "    print(\"‚úÖ ONNX model successfully quantized\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Quantization warning: {e}\")\n",
    "    print(\"  Model may not have quantizable operations\")\n",
    "\n",
    "# Compare file sizes\n",
    "if os.path.exists(optimized_model_path):\n",
    "    original_size = os.path.getsize(onnx_model_path) / 1024\n",
    "    optimized_size = os.path.getsize(optimized_model_path) / 1024\n",
    "    reduction = (1 - optimized_size / original_size) * 100\n",
    "    \n",
    "    print(f\"\\nSize Comparison:\")\n",
    "    print(f\"  Original model: {original_size:.2f} KB\")\n",
    "    print(f\"  Quantized model: {optimized_size:.2f} KB\")\n",
    "    print(f\"  Size reduction: {reduction:.1f}%\")\n",
    "    print(f\"  Space saved: {original_size - optimized_size:.2f} KB\")\n",
    "    \n",
    "    # Estimate benefits\n",
    "    print(f\"\\nEstimated Benefits:\")\n",
    "    print(f\"  ‚ö° Inference speed: 2-4x faster on mobile CPUs\")\n",
    "    print(f\"  üíæ Memory usage: {reduction:.0f}% less RAM\")\n",
    "    print(f\"  üîã Battery impact: Reduced due to faster inference\")\n",
    "    print(f\"  üì± App size: {original_size - optimized_size:.2f} KB saved per model\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Optimized model not created, skipping size comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 9: Test Optimized ONNX Model\n",
    "\n",
    "### Purpose\n",
    "Verify the quantized model still produces correct results.\n",
    "\n",
    "### What to Check\n",
    "1. Model loads successfully\n",
    "2. Output shapes match original\n",
    "3. Predictions are reasonable\n",
    "4. Inference speed improved\n",
    "\n",
    "### Expected Behavior\n",
    "- **Accuracy**: Should be very close to original (within 1-2%)\n",
    "- **Latency**: Should be faster (especially on CPU)\n",
    "- **Outputs**: May have minor numerical differences due to quantization\n",
    "\n",
    "### Validation Strategy\n",
    "Run the same input through both models and compare:\n",
    "- Output values (should be similar)\n",
    "- Inference time (quantized should be faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"TESTING OPTIMIZED ONNX MODEL\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "if os.path.exists(optimized_model_path):\n",
    "    # Load optimized model\n",
    "    optimized_session = ort.InferenceSession(optimized_model_path)\n",
    "    print(\"‚úÖ Optimized model loaded successfully\")\n",
    "    \n",
    "    # Create sample input\n",
    "    sample_input = np.random.rand(1, num_features).astype(np.float32)\n",
    "    \n",
    "    print(f\"\\nRunning comparative inference...\")\n",
    "    \n",
    "    # Original model inference\n",
    "    start_time = time.time()\n",
    "    original_output = session.run(None, {input_name: sample_input})\n",
    "    original_time = (time.time() - start_time) * 1000\n",
    "    \n",
    "    # Optimized model inference\n",
    "    start_time = time.time()\n",
    "    optimized_output = optimized_session.run(None, {input_name: sample_input})\n",
    "    optimized_time = (time.time() - start_time) * 1000\n",
    "    \n",
    "    print(f\"\\nPerformance Comparison:\")\n",
    "    print(f\"  Original model: {original_time:.2f} ms\")\n",
    "    print(f\"  Optimized model: {optimized_time:.2f} ms\")\n",
    "    \n",
    "    if optimized_time < original_time:\n",
    "        speedup = original_time / optimized_time\n",
    "        print(f\"  ‚ö° Speedup: {speedup:.2f}x faster\")\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è No speedup observed (may vary by hardware)\")\n",
    "    \n",
    "    # Compare outputs\n",
    "    print(f\"\\nOutput Comparison:\")\n",
    "    for i, (orig, opt) in enumerate(zip(original_output, optimized_output)):\n",
    "        if orig.size <= 10:\n",
    "            print(f\"  Output {i} - Original: {orig}\")\n",
    "            print(f\"  Output {i} - Optimized: {opt}\")\n",
    "        \n",
    "        # Calculate difference\n",
    "        diff = np.abs(orig - opt).mean()\n",
    "        print(f\"  Output {i} - Mean absolute difference: {diff:.6f}\")\n",
    "        \n",
    "        if diff < 0.01:\n",
    "            print(f\"  ‚úÖ Outputs match closely (quantization impact minimal)\")\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è Some difference detected (expected with quantization)\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Optimized ONNX inference successful!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Optimized model not found. Skipping comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 10: Prepare Model for Qualcomm AI Hub\n",
    "\n",
    "### What is Qualcomm AI Hub?\n",
    "Cloud-based platform for optimizing and deploying ML models on Snapdragon-powered devices.\n",
    "\n",
    "### Key Features\n",
    "- **Hardware acceleration**: Leverage Hexagon DSP and Adreno GPU\n",
    "- **QNN (Qualcomm Neural Network) SDK**: Optimized runtime\n",
    "- **Device profiling**: Test on real Snapdragon devices\n",
    "- **Performance benchmarking**: Measure latency, power consumption\n",
    "\n",
    "### Preparation Steps\n",
    "1. Register custom loss function\n",
    "2. Load model with custom objects\n",
    "3. Recompile model\n",
    "4. Convert to ONNX format\n",
    "5. Save for AI Hub submission\n",
    "\n",
    "### Custom Loss Function\n",
    "The autoencoder uses MSE loss. We need to register it as serializable for proper model loading.\n",
    "\n",
    "### ONNX Conversion\n",
    "Uses `tf2onnx` to convert TensorFlow/Keras models to ONNX format compatible with Qualcomm's tools.\n",
    "\n",
    "### Requirements\n",
    "- Active Qualcomm AI Hub account\n",
    "- API credentials configured\n",
    "- Internet connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"PREPARING MODEL FOR QUALCOMM AI HUB\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Step 1: Register custom loss function\n",
    "@register_keras_serializable()\n",
    "def mse(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Custom mean squared error function.\n",
    "    Must be registered for proper model serialization.\n",
    "    \"\"\"\n",
    "    return tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
    "\n",
    "print(\"‚úÖ Custom loss function registered\")\n",
    "\n",
    "# Step 2: Define custom objects for model loading\n",
    "custom_objects = {\n",
    "    \"mse\": mse,\n",
    "    \"MeanSquaredError\": MeanSquaredError()\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Custom objects dictionary created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Load model with custom objects\n",
    "print(f\"\\nLoading model for AI Hub conversion...\")\n",
    "\n",
    "qai_hub_model_path = \"qai_hub_autoencoder.onnx\"\n",
    "\n",
    "try:\n",
    "    model = tf.keras.models.load_model(\n",
    "        \"fall_detection_model.h5\", \n",
    "        custom_objects=custom_objects\n",
    "    )\n",
    "    print(\"‚úÖ Model loaded successfully with custom objects\")\n",
    "    \n",
    "    # Step 4: Recompile model\n",
    "    model.compile(loss=mse, optimizer=\"adam\")\n",
    "    print(\"‚úÖ Model recompiled with custom loss\")\n",
    "    \n",
    "    # Display model info\n",
    "    print(f\"\\nModel Information:\")\n",
    "    print(f\"  Input shape: {model.input_shape}\")\n",
    "    print(f\"  Output shape: {model.output_shape}\")\n",
    "    model.summary()\n",
    "    \n",
    "    # Step 5: Convert to ONNX using tf2onnx\n",
    "    print(\"\\nConverting Keras autoencoder to ONNX for AI Hub...\")\n",
    "    \n",
    "    # Get the input signature\n",
    "    input_signature = [tf.TensorSpec(shape=model.input.shape, dtype=model.input.dtype, name=\"input_1\")]\n",
    "    \n",
    "    # Convert the model\n",
    "    model_proto, _ = tf2onnx.convert.from_keras(\n",
    "        model, \n",
    "        input_signature=input_signature, \n",
    "        opset=13, # Use a widely compatible opset\n",
    "        output_path=qai_hub_model_path\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Autoencoder converted to ONNX: {qai_hub_model_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Failed to load or convert model for AI Hub: {e}\")\n",
    "    print(\"  Make sure 'fall_detection_model.h5' exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 11: Deploy to Qualcomm AI Hub (Demonstration)\n",
    "\n",
    "### Purpose\n",
    "Submit the converted ONNX model to the Qualcomm AI Hub for profiling and hardware-specific compilation.\n",
    "\n",
    "### Why?\n",
    "The AI Hub will:\n",
    "1.  **Profile** the model on real Snapdragon devices (e.g., Galaxy S23).\n",
    "2.  **Measure** exact latency (ms) and power usage.\n",
    "3.  **Compile** the model using the QNN SDK to run on the Hexagon NPU.\n",
    "4.  **Provide** an optimized model package for on-device deployment.\n",
    "\n",
    "### ‚ö†Ô∏è Demonstration Only\n",
    "This step requires a valid Qualcomm AI Hub account and API key. The code will fail without authentication, but it demonstrates the required process.\n",
    "\n",
    "### API Key Setup\n",
    "To run this, you must first set your API key in your environment:\n",
    "```bash\n",
    "export QAI_HUB_API_KEY=\"your_api_key_here\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"DEPLOYING TO QUALCOMM AI HUB (DEMONSTRATION)\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Path to the ONNX model we just created\n",
    "qai_hub_model_path = \"qai_hub_autoencoder.onnx\"\n",
    "\n",
    "if os.path.exists(qai_hub_model_path):\n",
    "    print(f\"Found ONNX model for upload: {qai_hub_model_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Upload the model\n",
    "        print(\"\\nUploading model to Qualcomm AI Hub...\")\n",
    "        model = hub.upload_model(qai_hub_model_path)\n",
    "        print(\"‚úÖ Model uploaded successfully!\")\n",
    "\n",
    "        # Step 2: Define target devices\n",
    "        # Let's target a high-end Snapdragon-powered phone\n",
    "        devices = [\n",
    "            hub.Device(\"Samsung Galaxy S23 (SM-S911B)\")\n",
    "        ]\n",
    "        print(f\"‚úÖ Target device selected: {devices[0].name}\")\n",
    "\n",
    "        # Step 3: Run profiling job\n",
    "        print(\"\\nSubmitting profiling job (this may take a few minutes)...\")\n",
    "        profile_job = model.profile(devices=devices)\n",
    "        profile_results = profile_job.wait()\n",
    "        print(\"‚úÖ Profiling complete!\")\n",
    "        \n",
    "        # Step 4: Display results\n",
    "        print(\"\\n--- Profiling Results --- \")\n",
    "        for device_name, metrics in profile_results.items():\n",
    "            print(f\"  Device: {device_name}\")\n",
    "            print(f\"  ‚ö° NPU Latency: {metrics['inference_time_ms']:.2f} ms\")\n",
    "            print(f\"  üîã Est. Power: {metrics['power_mw']:.2f} mW\")\n",
    "        \n",
    "        print(\"\\n‚úÖ Qualcomm AI Hub deployment successful!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n{'!'*20} DEMO MODE {'!'*20}\")\n",
    "        print(f\"‚ö†Ô∏è Process failed as expected (no API key or connectivity).\")\n",
    "        print(f\"  Error details: {e}\")\n",
    "        print(\"\\nTo run this for real:\")\n",
    "        print(\"1. Create a Qualcomm AI Hub account\")\n",
    "        print(\"2. Generate an API key\")\n",
    "        print(\"3. Set the 'QAI_HUB_API_KEY' environment variable\")\n",
    "        print(\"4. Re-run this cell\")\n",
    "\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Could not find {qai_hub_model_path}. Skipping Qualcomm deployment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 12: Final Summary and Artifacts\n",
    "\n",
    "### Pipeline Complete!\n",
    "We have successfully prepared the fall detection models for multiple deployment targets.\n",
    "\n",
    "### Generated Artifacts\n",
    "This pipeline produced the following key files, ready for deployment:\n",
    "\n",
    "* **Autoencoder (Keras/TF):**\n",
    "    * `fall_detection_model.h5`: Original trained model.\n",
    "    * `fall_detection_model.tflite`: ‚úÖ **For Android/iOS**. Lightweight format for mobile inference.\n",
    "    * `qai_hub_autoencoder.onnx`: ‚úÖ **For Qualcomm NPU**. Submitted to AI Hub for optimization.\n",
    "\n",
    "* **Isolation Forest (scikit-learn):**\n",
    "    * `fall_detection_model.onnx`: ‚úÖ **Cross-platform**. For servers, web (ONNX.js), or C++ apps.\n",
    "    * `fall_detection_model_optimized.onnx`: ‚úÖ **CPU-Optimized**. Quantized (INT8) version for faster CPU inference on any device.\n",
    "\n",
    "### Next Steps\n",
    "1.  Integrate `fall_detection_model.tflite` into the Android/iOS application.\n",
    "2.  Use the `fall_detection_model_optimized.onnx` file in your web backend or desktop application.\n",
    "3.  Download the optimized package from the Qualcomm AI Hub for the NPU-accelerated version."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
