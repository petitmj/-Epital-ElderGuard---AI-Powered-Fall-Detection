{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fall Detection Pipeline - Training & Preprocessing\n",
    "\n",
    "This notebook implements a comprehensive machine learning pipeline for fall detection using sensor data. It includes:\n",
    "- Data preprocessing and normalization\n",
    "- Dimensionality reduction using PCA\n",
    "- Random Forest classification\n",
    "- Hybrid anomaly detection (Isolation Forest + Autoencoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import all necessary libraries for data manipulation, machine learning, deep learning, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Configure logging to track pipeline execution\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Data Loading Function\n",
    "\n",
    "This function loads the fall detection dataset from a CSV file. The dataset should contain sensor readings (accelerometer, gyroscope, etc.) with activity labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filepath):\n",
    "    \"\"\"\n",
    "    Load fall detection dataset from CSV file.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to the CSV file containing sensor data and labels\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with sensor features and activity labels\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Data Preprocessing Function\n",
    "\n",
    "This function performs essential preprocessing steps:\n",
    "- Checks for missing values\n",
    "- Analyzes class distribution to identify imbalances\n",
    "- Applies StandardScaler normalization to ensure all features have zero mean and unit variance\n",
    "- This normalization is crucial for distance-based algorithms and neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Preprocess the dataset by checking data quality and normalizing features.\n",
    "    \n",
    "    Args:\n",
    "        df: Raw dataframe with sensor readings and labels\n",
    "    \n",
    "    Returns:\n",
    "        df_scaled: Normalized dataframe with scaled features\n",
    "        scaler: Fitted StandardScaler object for later use in inference\n",
    "    \"\"\"\n",
    "    # Check for missing values\n",
    "    missing_values = df.isnull().sum().sum()\n",
    "    logging.info(f\"Total missing values: {missing_values}\")\n",
    "    \n",
    "    # Check class distribution to identify potential class imbalance\n",
    "    class_counts = df['label'].value_counts()\n",
    "    logging.info(f\"Class Distribution:\\n{class_counts}\")\n",
    "    \n",
    "    # Normalize Data using Standard Scaling (z-score normalization)\n",
    "    scaler = StandardScaler()\n",
    "    features = df.iloc[:, 1:]  # Exclude label column (assumed to be first column)\n",
    "    scaled_features = scaler.fit_transform(features)\n",
    "    \n",
    "    # Convert back to DataFrame for easier manipulation\n",
    "    df_scaled = pd.DataFrame(scaled_features, columns=df.columns[1:])\n",
    "    df_scaled['label'] = df['label']  # Retain original labels\n",
    "    \n",
    "    return df_scaled, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define PCA Dimensionality Reduction Function\n",
    "\n",
    "Principal Component Analysis (PCA) reduces feature dimensionality while retaining 95% of the variance. This:\n",
    "- Reduces computational cost\n",
    "- Removes multicollinearity\n",
    "- Helps prevent overfitting\n",
    "- Speeds up model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca(df_scaled):\n",
    "    # Feature Selection using PCA (Dimensionality Reduction)\n",
    "    pca = PCA(n_components=0.95)  # Retain 95% variance\n",
    "    principal_components = pca.fit_transform(df_scaled.iloc[:, :-1])  # Exclude label\n",
    "    \n",
    "    # Convert PCA results into a DataFrame\n",
    "    df_pca = pd.DataFrame(principal_components)\n",
    "    df_pca['label'] = df_scaled['label']\n",
    "    \n",
    "    logging.info(f\"Original feature count: {df_scaled.shape[1] - 1}\")\n",
    "    logging.info(f\"Reduced feature count after PCA: {df_pca.shape[1] - 1}\")\n",
    "    \n",
    "    return df_pca, pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define Visualization Function\n",
    "\n",
    "Visualize the distribution of different activity classes to understand class balance and potential biases in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_class_distribution(class_counts):\n",
    "    \"\"\"\n",
    "    Create a bar plot showing the distribution of activity classes.\n",
    "    \n",
    "    Args:\n",
    "        class_counts: Series containing counts for each activity class\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.barplot(x=class_counts.index, y=class_counts.values, palette=\"viridis\")\n",
    "    plt.title(\"Class Distribution\")\n",
    "    plt.xlabel(\"Activity Type\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Define Random Forest Training Function\n",
    "\n",
    "Random Forest is an ensemble learning method that creates multiple decision trees and combines their predictions. It's robust to overfitting and handles high-dimensional data well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_random_forest(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Train a Random Forest classifier for activity classification.\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features\n",
    "        y_train: Training labels\n",
    "    \n",
    "    Returns:\n",
    "        model: Trained RandomForestClassifier\n",
    "    \"\"\"\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Define Model Evaluation Function\n",
    "\n",
    "Evaluate the trained model on test data and compute accuracy metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate model performance on test data.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained classifier\n",
    "        X_test: Test features\n",
    "        y_test: True test labels\n",
    "    \n",
    "    Returns:\n",
    "        accuracy: Model accuracy score\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    logging.info(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Define Model Saving Function\n",
    "\n",
    "Save the trained model and preprocessing objects for later use in production/inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_artifacts(model, scaler, label_encoder):\n",
    "    \"\"\"\n",
    "    Save trained model and preprocessing objects to disk.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained classifier\n",
    "        scaler: Fitted StandardScaler\n",
    "        label_encoder: Fitted LabelEncoder\n",
    "    \"\"\"\n",
    "    joblib.dump(model, \"fall_detection_model.pkl\")\n",
    "    joblib.dump(scaler, \"scaler.pkl\")\n",
    "    joblib.dump(label_encoder, \"label_encoder.pkl\")\n",
    "    logging.info(\"✅ Model and encoders saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Define Hybrid Anomaly Detection Model\n",
    "\n",
    "This advanced function combines two anomaly detection approaches:\n",
    "- **Isolation Forest**: Identifies anomalies by isolating observations in tree structures\n",
    "- **Autoencoder**: Neural network that learns to reconstruct normal patterns; poor reconstruction indicates anomalies\n",
    "\n",
    "The hybrid approach uses majority voting to combine predictions, improving robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hybrid_model(data, labels):\n",
    "    try:\n",
    "        scaler = StandardScaler()\n",
    "        scaled_data = scaler.fit_transform(data)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Isolation Forest\n",
    "        isolation_model = IsolationForest(contamination=0.05, random_state=42)\n",
    "        isolation_model.fit(X_train)\n",
    "        y_pred_iso = isolation_model.predict(X_test)\n",
    "        y_pred_iso = np.where(y_pred_iso == -1, 1, 0)\n",
    "\n",
    "        # Autoencoder\n",
    "        input_dim = X_train.shape[1]\n",
    "        autoencoder = keras.Sequential([\n",
    "            layers.Input(shape=(input_dim,)),\n",
    "            layers.Dense(16, activation=\"relu\"),\n",
    "            layers.Dense(8, activation=\"relu\"),\n",
    "            layers.Dense(16, activation=\"relu\"),\n",
    "            layers.Dense(input_dim, activation=\"linear\"),\n",
    "        ])\n",
    "\n",
    "        autoencoder.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "        autoencoder.fit(X_train, X_train, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "        reconstructions = autoencoder.predict(X_test)\n",
    "        mse = np.mean(np.power(X_test - reconstructions, 2), axis=1)\n",
    "        threshold = np.percentile(mse, 95)\n",
    "        y_pred_auto = (mse > threshold).astype(int)\n",
    "\n",
    "        # Hybrid Prediction (Majority Voting)\n",
    "        y_pred_hybrid = (y_pred_iso + y_pred_auto) >= 1\n",
    "        y_pred_hybrid = y_pred_hybrid.astype(int)\n",
    "\n",
    "        unique_labels = np.unique(y_test)\n",
    "        average_mode = \"binary\" if len(unique_labels) <= 2 else \"weighted\"\n",
    "\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            y_test, y_pred_hybrid, average=average_mode, zero_division=0\n",
    "        )\n",
    "\n",
    "        logging.info(f\"Hybrid Model - Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
    "\n",
    "        # Save models\n",
    "        joblib.dump(isolation_model, \"isolation_forest.pkl\")\n",
    "        joblib.dump(scaler, \"scaler.pkl\")\n",
    "        autoencoder.save(\"autoencoder.h5\")\n",
    "\n",
    "        return isolation_model, autoencoder, scaler\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error training Hybrid Model: {e}\")\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Load and Preprocess Dataset\n",
    "\n",
    "Execute the data loading and preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (5000, 29)\n",
      "\n",
      "First few rows:\n",
      "          label  acc_x_mean  acc_y_mean  acc_z_mean  acc_x_std  acc_y_std  \\\n",
      "0       sitting      0.0351      0.0344      9.6897     0.1499     0.1204   \n",
      "1  fall_forward      2.3267     -1.4092     12.2534     8.5069     7.9289   \n",
      "2       walking      0.1272      0.2537      9.7369     1.6084     0.9839   \n",
      "3       jogging      0.4990      0.6269     10.6038     3.0459     3.8435   \n",
      "4       walking      0.1823      0.2419      9.7604     1.4799     1.2406   \n",
      "\n",
      "   acc_z_std  acc_x_min  acc_y_min  acc_z_min  ...  acc_magnitude_mean  \\\n",
      "0     0.2097    -0.1649    -0.1656     9.3897  ...              9.9907   \n",
      "1     8.1094    -6.6733    -9.4092     2.2534  ...             25.6781   \n",
      "2     1.5309    -1.3728    -1.2463     7.7369  ...              9.6984   \n",
      "3     3.0127    -2.5010    -2.8731     7.6038  ...             13.2501   \n",
      "4     2.0045    -1.3177    -1.2581     7.7604  ...              9.7145   \n",
      "\n",
      "   acc_magnitude_std  gyro_magnitude_mean  gyro_magnitude_std  acc_jerk_mean  \\\n",
      "0             0.4799               0.0276              0.0770         0.2009   \n",
      "1            24.5451               7.6790             10.0382        42.2947   \n",
      "2             4.1232               0.3234              0.8045         2.7113   \n",
      "3             9.9022               1.1674              2.0414         7.3108   \n",
      "4             4.7249               0.3280              0.9473         2.4673   \n",
      "\n",
      "   gyro_jerk_mean  pitch_mean  roll_mean  activity_duration  peak_acceleration  \n",
      "0          0.0512     -0.0745     0.7095             5.8268            10.6026  \n",
      "1         27.0994     61.9148    21.8163             0.6921            53.8429  \n",
      "2          0.8429      4.6700     4.0684             1.0716            15.0058  \n",
      "3          2.8074      7.7915     9.3633             1.1666            32.4665  \n",
      "4          0.7621      4.6885     4.5338             1.0403            18.8251  \n",
      "\n",
      "[5 rows x 29 columns]\n",
      "\n",
      "Column names:\n",
      "['label', 'acc_x_mean', 'acc_y_mean', 'acc_z_mean', 'acc_x_std', 'acc_y_std', 'acc_z_std', 'acc_x_min', 'acc_y_min', 'acc_z_min', 'acc_x_max', 'acc_y_max', 'acc_z_max', 'gyro_x_mean', 'gyro_y_mean', 'gyro_z_mean', 'gyro_x_std', 'gyro_y_std', 'gyro_z_std', 'acc_magnitude_mean', 'acc_magnitude_std', 'gyro_magnitude_mean', 'gyro_magnitude_std', 'acc_jerk_mean', 'gyro_jerk_mean', 'pitch_mean', 'roll_mean', 'activity_duration', 'peak_acceleration']\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "file_path = \"fall_detection_dataset.csv\" \n",
    "df = load_dataset(file_path)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\\n{df.head()}\")\n",
    "print(f\"\\nColumn names:\\n{df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Apply Preprocessing and PCA\n",
    "\n",
    "Normalize the features and reduce dimensionality using PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-21 13:38:58,897 - INFO - Total missing values: 0\n",
      "2025-12-21 13:38:58,962 - INFO - Class Distribution:\n",
      "label\n",
      "walking          1500\n",
      "standing         1000\n",
      "sitting           750\n",
      "lying             500\n",
      "stairs_up         350\n",
      "stairs_down       300\n",
      "jogging           250\n",
      "fall_forward      100\n",
      "fall_backward     100\n",
      "fall_sideways      75\n",
      "syncope            50\n",
      "trip               25\n",
      "Name: count, dtype: int64\n",
      "2025-12-21 13:38:59,148 - INFO - Original feature count: 28\n",
      "2025-12-21 13:38:59,150 - INFO - Reduced feature count after PCA: 4\n",
      "2025-12-21 13:38:59,204 - INFO - ✅ Preprocessed data saved as 'processed_data.csv'\n"
     ]
    }
   ],
   "source": [
    "# Preprocess data\n",
    "df_scaled, scaler = preprocess_data(df)\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "df_pca, pca = apply_pca(df_scaled)\n",
    "\n",
    "# Save preprocessed data\n",
    "df_pca.to_csv(\"processed_data.csv\", index=False)\n",
    "logging.info(\"✅ Preprocessed data saved as 'processed_data.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Encode Labels and Split Data\n",
    "\n",
    "Convert text labels to numerical format and split data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 4000\n",
      "Testing samples: 1000\n"
     ]
    }
   ],
   "source": [
    "# Encode categorical labels to numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "df_pca['label'] = label_encoder.fit_transform(df_pca['label'])\n",
    "\n",
    "# Split dataset into features and labels\n",
    "X = df_pca.drop(columns=[\"label\"])\n",
    "y = df_pca[\"label\"]\n",
    "\n",
    "# Split into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Testing samples: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Train and Evaluate Random Forest Model\n",
    "\n",
    "Train the Random Forest classifier and evaluate its performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-21 13:39:00,006 - INFO - Model Accuracy: 0.9910\n",
      "2025-12-21 13:39:00,072 - INFO - ✅ Model and encoders saved successfully!\n"
     ]
    }
   ],
   "source": [
    "    # Train and evaluate Random Forest model\n",
    "    rf_model = train_random_forest(X_train, y_train)\n",
    "    evaluate_model(rf_model, X_test, y_test)\n",
    "    save_model_artifacts(rf_model, scaler, label_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Train and Evaluate Hybrid Anomaly Detection Model\n",
    "\n",
    "Train the hybrid model combining Isolation Forest and Autoencoder for improved anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python313\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:171: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  return np.array(x)\n",
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python313\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:171: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  return np.array(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-21 13:39:20,063 - INFO - Hybrid Model - Precision: 0.0034, Recall: 0.0120, F1-Score: 0.0053\n",
      "2025-12-21 13:39:20,106 - WARNING - You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python313\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:171: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  return np.array(x)\n",
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python313\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:171: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  return np.array(x)\n",
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python313\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:171: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  return np.array(x)\n",
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python313\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:171: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  return np.array(x)\n",
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python313\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:171: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  return np.array(x)\n",
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python313\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:171: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  return np.array(x)\n",
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python313\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:171: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  return np.array(x)\n",
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python313\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:171: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  return np.array(x)\n",
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python313\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:171: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  return np.array(x)\n",
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python313\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:171: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  return np.array(x)\n",
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python313\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:171: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  return np.array(x)\n",
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python313\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:171: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  return np.array(x)\n",
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python313\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:171: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  return np.array(x)\n",
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python313\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:171: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  return np.array(x)\n",
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python313\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:171: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  return np.array(x)\n",
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python313\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:171: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  return np.array(x)\n",
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python313\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:171: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  return np.array(x)\n",
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python313\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:171: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  return np.array(x)\n",
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python313\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:171: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  return np.array(x)\n",
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python313\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:171: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  return np.array(x)\n",
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python313\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:171: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  return np.array(x)\n",
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python313\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:171: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  return np.array(x)\n",
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python313\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:171: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  return np.array(x)\n",
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python313\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:171: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  return np.array(x)\n",
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python313\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:171: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  return np.array(x)\n",
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python313\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:171: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  return np.array(x)\n",
      "2025-12-21 13:39:20,274 - INFO - ✅ Hybrid Model training completed!\n"
     ]
    }
   ],
   "source": [
    "# Train hybrid model\n",
    "isolation_model, autoencoder, hybrid_scaler = train_hybrid_model(X, y)\n",
    "\n",
    "if isolation_model is not None:\n",
    "    logging.info(\"✅ Hybrid Model training completed!\")\n",
    "else:\n",
    "    logging.error(\"❌ Hybrid Model training failed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. DuckDuckGo Care Assistance Integration\n",
    "\n",
    "To complement the sensor-driven fall detection pipeline, the project now exposes caregiver assistance utilities backed by DuckDuckGo:\n",
    "\n",
    "- **Health Information Assistance:** surfaces fall-prevention checklists, recovery guidance, and emergency contact tips tailored to elderly caregivers.\n",
    "- **Emergency / Expert Locator:** highlights reputable organizations or specialists that can support elderly patients post-fall.\n",
    "\n",
    "These helpers now call the **DuckDuckGo Instant Answer API** directly (no third-party proxy). The Instant Answer endpoint returns structured abstracts and related topics from trusted sources, which we normalize via `duckduckgo_service.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DuckDuckGo Instant Answer helpers imported successfully.\n"
     ]
    }
   ],
   "source": [
    "# Import DuckDuckGo helper utilities (Instant Answer API-backed)\n",
    "from duckduckgo_service import (\n",
    "    fetch_health_information,\n",
    "    locate_emergency_facilities,\n",
    "    DuckDuckGoIntegrationError,\n",
    ")\n",
    "\n",
    "print(\"DuckDuckGo Instant Answer helpers imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.1 Fetch Health Information Suggestions\n",
    "`fetch_health_information` now queries the DuckDuckGo Instant Answer API.\n",
    "It surfaces trustworthy abstracts and related topics (e.g., Wikipedia or\n",
    "government health portals) without relying on any third-party scraper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\Documents\\Projects\\-Epital-ElderGuard---AI-Powered-Fall-Detection\\duckduckgo_service.py:34: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
      "  with DDGS() as ddgs:\n",
      "2025-12-21 13:39:22,560 - INFO - response: https://www.bing.com/search?q=fall+prevention+exercises+tips+for+home+caregivers 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[InfoCard(title='fall的用法_百度知道', snippet='fall 高考 / CET4 / CET6 / 考研 讲解1:26 v. 落下；下落；掉落；跌落；突然倒下；跌倒；倒塌；下垂；低垂 n. 落下；下落；跌落；掉落； (雪、岩石等的)降落； …', url='https://zhidao.baidu.com/question/589864611.html', metadata=None),\n",
       " InfoCard(title='fall 和 fell 有什么区别?_百度知道', snippet='fall 和 fell 只有时态的区别，没有意思上的区别，只是以下几点需要注意： fall是一般现在时，而fell是一般过去时，时态不同； fell 动词原形的时候，表示砍 …', url='https://zhidao.baidu.com/question/1768944762480158620.html', metadata=None),\n",
       " InfoCard(title='“autumn”跟“fall”有什么区别? - 百度知道', snippet='“autumn”跟“fall”有什么区别?autumn和fall这两个词的区别在于使用地区的不同，前者为英国用词，后者是美国英语。 1、autumn是比较正式的书面用语，是英 …', url='https://zhidao.baidu.com/question/629770527425317564.html', metadata=None)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: caregivers looking for fall-prevention routines\n",
    "try:\n",
    "    fall_prevention_cards = fetch_health_information(\n",
    "        topic=\"fall prevention exercises\",\n",
    "        audience=\"home caregivers\",\n",
    "        max_results=3,\n",
    "    )\n",
    "except DuckDuckGoIntegrationError as err:\n",
    "    print(f\"DuckDuckGo Instant Answer failed: {err}\")\n",
    "    fall_prevention_cards = []\n",
    "\n",
    "fall_prevention_cards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.2 Locate Post-Fall Medical Experts\n",
    "`locate_emergency_facilities` also leverages the Instant Answer API, which tends\n",
    "to surface curated articles or organization pages (e.g., geriatric care\n",
    "associations, government safety guidance) rather than raw map coordinates.\n",
    "\n",
    "The example below demonstrates a targeted query for medical specialists and\n",
    "programs that support elderly patients after a fall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\Documents\\Projects\\-Epital-ElderGuard---AI-Powered-Fall-Detection\\duckduckgo_service.py:34: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
      "  with DDGS() as ddgs:\n",
      "2025-12-21 13:39:24,587 - INFO - response: https://www.bing.com/search?q=geriatric+rehabilitation+specialists+site%3A.org+post-fall+care+tips+for+caregivers 200\n",
      "2025-12-21 13:39:26,246 - INFO - response: https://www.bing.com/search?q=geriatric+rehabilitation+specialists+site%3A.org+post-fall+care+tips+for+caregivers&first=11&FORM=PERE 200\n",
      "2025-12-21 13:39:27,679 - INFO - response: https://www.bing.com/search?q=geriatric+rehabilitation+specialists+site%3A.org+post-fall+care+tips+for+caregivers&first=21&FORM=PERE1 200\n",
      "2025-12-21 13:39:29,197 - INFO - response: https://www.bing.com/search?q=geriatric+rehabilitation+specialists+site%3A.org+post-fall+care+tips+for+caregivers&first=31&FORM=PERE2 200\n",
      "2025-12-21 13:39:30,600 - INFO - response: https://www.bing.com/search?q=geriatric+rehabilitation+specialists+site%3A.org+post-fall+care+tips+for+caregivers&first=41&FORM=PERE3 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: find geriatric medical experts for post-fall care\n",
    "try:\n",
    "    geriatric_experts = fetch_health_information(\n",
    "        topic=\"geriatric rehabilitation specialists site:.org post-fall care\",\n",
    "        audience=\"caregivers\",\n",
    "        max_results=3,\n",
    "    )\n",
    "except DuckDuckGoIntegrationError as err:\n",
    "    print(f\"DuckDuckGo expert lookup failed: {err}\")\n",
    "    geriatric_experts = []\n",
    "\n",
    "geriatric_experts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Summary and Next Steps\n",
    "\n",
    "**Models Trained:**\n",
    "1. Random Forest Classifier - For multi-class activity classification\n",
    "2. Hybrid Anomaly Detector - Combining Isolation Forest and Autoencoder\n",
    "\n",
    "**Saved Artifacts:**\n",
    "- `fall_detection_model.pkl` - Random Forest model\n",
    "- `isolation_forest.pkl` - Isolation Forest model\n",
    "- `autoencoder.h5` - Autoencoder neural network\n",
    "- `scaler.pkl` - Feature scaler\n",
    "- `label_encoder.pkl` - Label encoder\n",
    "- `processed_data.csv` - Preprocessed dataset\n",
    "\n",
    "**Next Steps:**\n",
    "- Use the inference pipeline notebook to convert models for mobile deployment\n",
    "- Test models on new sensor data\n",
    "- Deploy to edge devices for real-time fall detection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
